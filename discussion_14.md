## word2vec
- 用信息学院官网和公众号的所有新闻，和老师的姓名、专业训练了word2vec，效果比之前好很多，对比字典中的几类进行抽样，表现不错
- 速度不算快，训练和构建语料库各自都在1-2分钟
- 总共6440条新闻
- 所有学院新闻加起来，要得到语料库应该得并行，训练可能也一样，需要研究
- **停用词词典中是否需要去除年份，我认为需要**

## 数据处理
- szh和snl统一一下新闻的格式，列名都用英文，datetime,source,url,title,content五项内容，邵总的全写成信火相传公众号
- snl和szh把新闻内容处理一下

## 字典
- jieba可以识别出来会议的前缀

# 怎么把item的特征抽取出来
1. 抽取item属性
2. 学习用户画像
3. 过滤item，得到符合用户的item并排序

## 数据结构
- 文章：和现有差不多
- 用户：

## 分工
zpt：爬老师实验室，把现有的实体加入neo4j，找命名实体识别和关系抽取（从文本中找谁参加了什么会议），看信息学院新闻里有多少篇不涉及任何关键词

szh：实现基础的tf-idf，处理数据

snl：了解sql以及推荐系统如何存储用户数据、新闻数据等等，python能用的推荐系统的库

*融合关键词，字典作为关键词的字典*
层级划分来实现时效性
种子集可以先不考虑

1. 先用最基础的、不涉及关键词的tf-idf实现：
   - 考虑使用热度推荐，当冷启动时返还订阅标签下点击频率最高的新闻
2. 协同过滤：
   - 数据哪里来 


## 问题
- tf-idf处理时遇到新来的词汇怎么办（超过原有词典），需要重新训练？但用户向量是之前的维度表达，两者不一致
- 如果tf-idf向量过长怎么办
- 如何定义用户和新闻的数据结构，以及怎么把数据结构中的特定内容融合在向量表达中？

## 历史遗留问题
- 爬虫的更新

## ddl
8.6 晚上8点