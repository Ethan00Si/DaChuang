# 统一处理所有的新闻

## 爬虫

* 用翻页的方式，找到未爬取的新闻（比如：对比日期【和上次爬取的日期比，这样就不用读取数据库了】或者数据库是否有此url）
* 定期爬取，设定一个“闹钟”程序
* 最好是把所有的网站的爬取配置总结以后写成独立的配置文件，这样一个爬虫爬所有的网站
  * xyq的爬虫看起来是每个网站一个单独的爬虫。建议每个网站写一个json配置文件，或者像snl一样写一个.py的配置文件

## Preprocess

* 将要处理的新闻的名字添加到file_names.json中(按照相对路径写，默认相对于Dachuang的路径)
* 将对应的数据的处理方法按照标准放在ini文件中
  * "special_characters"是对content、title进行处理的时候，需要删除的特殊元素，默认只包含["\n", "\r", "NULL", "nan"]。 （' '也是默认处理）
    * 如果你的新闻有特殊字符请加入
      * 好像看到公众号的标题里混了一些奇怪的东西
    * ==划重点==  title需要认真处理一下，小程序要显示标题的，处理不好，影响用户的观感
  * date_position是日期在datatime中所处的位置 
    * 例如 '时间：2020-08-23' 对应(3,13) 
  * date_format是日期格式 
    * '2020-08-23'对应"%Y-%m-%d"，'2020/08/23'对应"%Y/%m/%d"



## 添加到数据库 

* 将新来的新闻添加到数据库

  * 添加的时候，要加art_type属性（数据库article表中的），写在ini下的json中

    * 命名的习惯按照data下的readme.md中的表格的名字写

    

  * ==问题==

    最后推荐的时候，假如小明关键词只选了info，就只推荐info 的新闻，所以只计算info新闻的得分就好了；而且假如有时间设置（例如：只推荐一个月内的新闻，那计算得分的时候一个月前的就直接不管了）

    * ==存储tf-idf矩阵的时候，需要知道对应的文档的来源==
      * ==所以tf-idf矩阵需要有和数据库中文章的对应关系==
      * ==如果要存到数据库，需要新开一张表，但是每个用户推荐都去访问表格，是不是运行速度会很慢啊（毕竟每次都有数据库操作）==
      * ==如果把tf-idf矩阵cache到内存里，不知道内存会不会不够，但是这样就可以只操作要返回的结果的数据库内容==



## 切词

* 切词的时候维护一个映射关系 id <——> 切词结果（通过数据库获得id）
  * 训练tf-idf矩阵的时候，通过这个关系得知每一行对应的是什么新闻

* 切词结果保存在

  data/语料/cut_words_result/result.json

  

## 计算tf-idf矩阵

默认：

1. 从data/语料/cut_words_result/result.json中读取切词结果

2. vocabualry recommender_system/CB/storage/vocabulary.json

​       vocabulary :   词汇 对应 列号  （eg：'北京市科委': 11190, '海淀区政府': 11191, '联谊会': 11192）

3. tfidf : 

​                    vocabulary

​             doc|-----------|

​       	         |             |

   metric:     |             |

   	             |             |

   	             |-----------|  

​        存储在 recommender_system/CB/storage/tfidf.npy

4. IDF： 返回结果是当前vocabulary的 每一列 对应的IDF

​        保存在 recommender_system/CB/storage/IDF.npy